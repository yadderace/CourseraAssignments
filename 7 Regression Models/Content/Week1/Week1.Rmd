---
title: "Regression Models Content"
author: "Yadder Aceituno"
date: "March 19, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Week 1

## Libraries

```{r libraries, warning=FALSE, message=FALSE}
library(UsingR)
library(ggplot2)
library(manipulate)
```

## Load Data

```{r load}
data(galton)
```

## Example 1

```{r example1}

## Without manipulate function
freqData <- as.data.frame(table(galton$child, galton$parent));
names(freqData) <- c("child", "parent", "freq");
plot(as.numeric(as.vector(freqData$parent)), 
      as.numeric(as.vector(freqData$child)), 
     pch = 21, col = "black", bg = "lightblue", 
     cex = .15 * freqData$freq, 
     xlab = "parent", ylab = "child");


## With manipulate function
myPlot <- function(beta){
  y <- galton$child - mean(galton$child)
  x <- galton$parent - mean(galton$parent)
  freqData <- as.data.frame(table(x, y))
  names(freqData) <- c("child", "parent", "freq")
  plot(
    as.numeric(as.vector(freqData$parent)), 
    as.numeric(as.vector(freqData$child)),
    pch = 21, col = "black", bg = "lightblue",
    cex = .15 * freqData$freq, 
    xlab = "parent", 
    ylab = "child")
  abline(0, beta, lwd = 3)
  points(0, 0, cex = 2, pch = 19)
  mse <- mean( (y - beta * x)^2 )
  title(paste("beta = ", beta, "mse = ", round(mse, 3)))
}

##Evaluation
##manipulate(myPlot(beta), beta = slider(0.6, 1.2, step = 0.02))

```

## Equations

### Variance
$$
s^2 =\frac{1}{(n-1)}\sum_{i=1}^n(x_i - \bar{x})^2 = \frac{1}{(n-1)} ( \sum_{i=1}^n x_i^2 - n\bar{x} ^2)
$$

### Standard Deviation
$$
s = \sqrt {s^2}
$$

### Normalizing Data
$$
z_i = \frac{x_i - \bar{x}}{s}
$$

### Covariance
It's a measure of the joint variability of two random variables. If the greater values of one variable mainly correspond with the greater values of the other variable, and the same holds for the lesser values, i.e., the variables tend to show similar behavior, the covariance is positive. 
In the opposite case, when the greater values of one variable mainly correspond to the lesser values of the other, i.e., the variables tend to show opposite behavior, the covariance is negative. 
$$
Cov(x,y) = \frac {1}{(n-1)} \sum_{i=1}^n (x_i-\bar{x})(y_i-\bar{y}) =  \frac {1}{(n-1)} (\sum_{i=1}^n x_iy_i - n\bar{x}\bar{y} )
$$

### Correlation 
A correlation coefficient is a numerical measure of some type of correlation, meaning a statistical relationship between two variables. The variables may be two columns of a given data set of observations, often called a sample, or two components of a multivariate random variable with a known distribution.
$$
Cor(x,y) = \frac{Cov(x,y)}{s_x s_y} = Cor(y,x)
$$
It's imporant to take note about:
$$
-1 \leqslant Cor(x,y) \leqslant 1 
$$
If
$$
Cor(x,y) = 0 \implies \text{No linear relation}
$$

Using least square we want to minimize the next sum
$$
\sum_{i=1}^n[y_i - (\beta_0 + \beta_1x_i)]^2
$$
We need to find beta0 and beta1 with this equations:
$$
\beta_1 = Cor(y,x)\frac{s_y}{s_x}
$$
$$
\beta_0 = \bar{y} - \beta_1 \bar{x}
$$

# Week 2

## Interpreting Coeficients
We have the probabilistic model for linear regression, including $\epsilon_i$ as our gaussian error.
$$
Y_i = \beta_0 + \beta_1 X_i + \epsilon_i
$$

Now, let's cosider this: 
$$
Y_i = \beta_0 + \beta_1 X_i + \epsilon_i = \beta_0 + a \beta_1 + \beta_1 (X_i - a) + \epsilon_i = \tilde{\beta_0} + \beta_1(X_1 - a) + \epsilon_i
$$
If we shift the regressor $(X_i - a)$, it doesn't change the slope $\beta_0$

Also, let's consider the next one:
$$
Y_i =  \beta_0 + \beta_1X_i + \epsilon_i = \beta_0 + \frac{\beta_1}{a}(X_ia) + \epsilon_i = \beta_0 + \tilde{\beta_1}(X_ia) + \epsilon_i
$$
Mutiplying $X$ by a factor $a$ results in dividing the coefficient by a factor of $a$.

## Example

### Diamond data set
Data is diamond prices (Singapore dollars) and diamond weight in carats (standard measure of diamond mass, 0.2 $g$)
```{r load_diamond_data}
library(UsingR);
library(ggplot2);

data(diamond);

g <- ggplot(diamond, aes(x = carat, y = price));
g <- g + xlab("Mass (carats)");
g <- g + ylab("Price (SIN $)");
g <- g + geom_point(size = 6, colour = "black", alpha = 0.2);
g <- g + geom_point(size = 5, colour = "blue", alpha = 0.2);
g <- g + geom_smooth(method = "lm", colour = "black");
g;
```

### Fitting the linear regression model
```{r}
fit <- lm(price ~ carat, data = diamond);
coef(fit);
```

***Our slope $\beta_1 = 3721.0249$ says, we estimate an expected  $3721.02$(SIN) dollar increase in price for every carat increase in mass of diamond.***

***The intercept $\beta_0 = -259.63$ is the expected price of a 0 carat diamond. (Not interesting)***

### Getting a more interpretable intercept
```{r}
fit2 <- lm(price ~ I(carat - mean(carat)), data = diamond);
coef(fit2);
```
We see that the slope $\beta_1 = 3721.0249$ stay the same, but the Intercept $\beta_0 = 500.1$ is the expected price for the average sized diamond of the data.
```{r}
mean(diamond$carat);
```

Now, suppose we need to change the units of carats by a factor of 10, so we know that we have to divide the $\beta_1$ by 10. Let's check that:

```{r}
fit3 <- lm(price ~ I(carat * 10), data = diamond);
coef(fit3);
```

We see that out $\beta_1 = 372.1025$ is equal to $3721.0249 / 10$.

### Predicting the price of a diamond
Imagine we have this three new caret diamonds
```{r}
newd <- c(0.16, 0.27, 0.34);
```

And we have 2 ways to predict the values:
```{r}
## Using the formula beta_0 + beta_1 * carat
coef(fit)[1] + coef(fit)[2] * newd;

## Using predict function
predict(fit, newdata = data.frame(carat = newd));
```

## Residuals
Our observed outcome $i$ is $Y_i$ at a predictor value $X_i$

The predicted outcome $i$ is $\hat{Y_i}$ at a predictor value $X_i$

The model:
$$
\hat{Y_i}=\hat{\beta_0} + \hat{\beta_1}X_i
$$

We can define the residuals with
$$
e_i = Y_i-\hat{Y_i}
$$

Least Squares minimizes: $\sum_{i=1}^ne_i^2$

### Example
```{r}
data(diamond);
y <- diamond$price;
x <- diamond$carat;
n <- length(y);
fit <- lm(y~x);

## Getting the residuals (easy way)
e<- resid(fit);

yhat <- predict(fit);

## Lets see the residuals
e;

## Lets see the predictions
yhat;
```

Now lets plot the residuals
```{r}
plot(diamond$carat, diamond$price,
     xlab = "Mass (carats)",
     ylab = "Price (SIN $)",
     bg = "lightblue",
     col = "black", cex = 1.1, pch = 21, frame = FALSE);
abline(fit, lwd = 2);
for(i in 1: n)
  lines(c(x[i],x[i]), c(y[i], yhat[i]), col = "red", lwd = 2);
```

Lets do another plot for residuals
```{r}
plot(x,e,
     xlab="Mass (carats)",
     ylab="Residuals (SIN $)",
     bg = "lightblue",
     col = "black", cex = 2, pch = 21, frame = FALSE);
abline(h = 0, lwd = 2);
for(i in 1: n)
  lines(c(x[i], x[i]), c(e[i],0), col="red", lwd = 2);
```

### Estimating Residual Variance
We can estimate $\sigma^2$ is equal to $\frac{1}{n}\sum_{i=1}^ne_i^2$ the averaged sqared residual

Most people use:
$$
\hat{\sigma}^2 = \frac{1}{n-2}\sum_{i=1}^ne_i^2
$$

The $n - 2$ instead of $n$ is so that $E[\hat{\sigma}^2] = \sigma^2$

```{r}
y <- diamond$price;
x <- diamond$carat;
n <- length(y);
fit <- lm(y~x);

## Checking the residual variation
summary(fit)$sigma;

## Calculating the residual variation
sqrt(sum(resid(fit)^2)/(n-2));
```

### Diamond data residual plot
```{r}
e <- c(resid(lm(price~1, data = diamond)),
       resid(lm(price~carat, data = diamond)));
fit <- factor(c(rep("Itc", nrow(diamond)),
                rep("Itc, slope", nrow(diamond))));
g <- ggplot(data.frame(e = e, fit = fit), aes(y = e, x = fit, fill = fit));
g <- g + geom_dotplot(binaxis = "y", stackdir = "center", binwidth = 20);
g <- g + xlab("Fitting approach");
g <- g + ylab("Residual Price");
g;
```

### Total Variability
The total variability in our response is the variability around an intercept $\sum_{i=1}^n(Y_i - \bar{Y})^2$
The regression variability is the variability that is explained by adding the predictor $\sum_{i=1}^n(\hat{Y_i}-\bar{Y})^2$
The error variability is what's leftover around the regression line $\sum_{i=1}^n(Y_i-\hat{Y_i})^2$

We have a neat fact:
$$
\sum_{i=1}^n(Y_i - \bar{Y})^2 = \sum_{i=1}^n(Y_i - \hat{Y_i})^2 + \sum_{i=1}^n(\hat{Y_i}-\bar{Y_i})^2
$$

R squared is the pecentage of the total variability that is explained by the linear relationship with the predictor:

$$
R^2 = \frac{\sum_{i=1}^n(\hat{Y_i}-\bar{Y})^2}{\sum_{i=1}^n(Y_i - \bar{Y})^2}
$$

### Heteroskedasticity
```{r}
x <- runif(100, 0, 6);
y <- x + rnorm(100, mean = 0, sd = 0.001 * x);
g <- ggplot(data.frame(x = x, y = y), aes(x = x, y = y));
g <- g + geom_smooth(method = "lm", colour = "black");
g <- g + geom_point(size = 7, colour= "black", alpha = 0.4);
g <- g + geom_point(size = 5, colour = "red", alpha = 0.4);
g;

## Getting rid of the blank space can be helpful
g <- ggplot(data.frame(x = x, y = resid(lm(y~x))), aes(x=x, y=y));
g <- g + geom_hline(yintercept = 0, size = 2);
g <- g + geom_point(size = 7, colour = "black", alpha = 0.4);
g <- g + geom_point(size = 5, colour = "red", alpha = 0.4);
g <- g + xlab("X") + ylab("Residual");
g;

```

## Inference in regression
### Review
Lets consider the model:
$$
Y_i = \beta_0 + \beta_1X_i +\epsilon_i
$$
$$
\hat{\beta_0} = \bar{Y} - \hat{\beta_1}\bar{X}
$$
$$
\hat{\beta_1} = Cor(Y,X)\frac{Sd(Y)}{Sd(X)}
$$

Review from statistical inference, can be used to test: $H_0: \theta = \theta_0$ versus $H_a: \theta >, <, \neq \theta_0$

Can be used to create a confidence interval for $\theta$ via $\hat{\theta} \pm Q_{1-\alpha/2}\hat{\sigma_{\hat{\theta}}}$ where $Q_{1-\alpha/2}$ is the relevant quantile from either a normal or T distribution.

### Results

Variance for the slope:
$$
\sigma_{\hat{\beta_1}}^2 = Var(\hat{\beta_1}) = \sigma^2 / \sum_{i=1}^n(X_i-\bar{X})^2
$$
The standard error for $\beta_1$:
$$
\sigma_{\hat{\beta_1}} = \sqrt{\sigma^2 / \sum_{i=1}^n(X_i-\bar{X})^2}
$$

Variance for the intercept:
$$
\sigma_{\hat{\beta_0}}^2 = Var(\hat{\beta_0}) = \bigg( \frac{1}{n} + \frac{\bar{X}^2}{\sum_{i=1}^n(X_i-\bar{X})^2}  \bigg) \sigma^2
$$

The standard error for $\beta_0$:
$$
\sigma_{\hat{\beta_0}} = \sqrt{\bigg( \frac{1}{n} + \frac{\bar{X}^2}{\sum_{i=1}^n(X_i-\bar{X})^2}  \bigg) \sigma^2}
$$

### Example
```{r}
library(UsingR);
data(diamond);
y <- diamond$price;
x <- diamond$carat;
n <- length(y);
beta1 <- cor(y,x) * sd(y) / sd(x);
beta0 <- mean(y) - beta1 * mean(x);
e <- y - beta0 - beta1 * x;
sigma <- sqrt(sum(e^2)/(n-2));
ssx <- sum((x - mean(x))^2);
seBeta0 <- ((1/n)+ mean(x)^2/ssx) ^ .5 * sigma;
seBeta1 <- sigma / sqrt(ssx);

## I assume that miu_beta0 and miu_beta1 is equal to 0
tBeta0 <- beta0 / seBeta0;
tBeta1 <- beta1 / seBeta1;

pBeta0 <- 2 * pt(abs(tBeta0), df = n - 2, lower.tail = FALSE);
pBeta1 <- 2 * pt(abs(tBeta1), df = n - 2, lower.tail = FALSE);

coefTable <- rbind(
  c(beta0, seBeta0, tBeta0, pBeta0),
  c(beta1, seBeta1, tBeta1, pBeta1));

colnames(coefTable) <- c("Estimate", "Std. Error", "t value", "P(>|t|)");
rownames(coefTable) <- c("(Intercept)", "x");


## Using lm function
fit <- lm(y ~ x);

## Comparing coefficients table
coefTable
summary(fit)$coefficients;
```

### Getting Confidence Interval
```{r}
coeffs <- summary(fit)$coefficients;
intervalBeta0 <- coeffs[1,1] + c(1,-1) * qt(0.975, df = fit$df) * coeffs[1,2];
intervalBeta1 <- coeffs[2,1] + c(1,-1) * qt(0.975, df = fit$df) * coeffs[2,2];

intervalBeta0;
intervalBeta1;
```

### Prediction of outcomes
The obvious estimate for prediction at point $x_0$ is $\hat{\beta_0} + \hat{\beta_1}x_0$, but a standard error is needed to create a prediction interval.

$x_0$ SE, $\hat{\sigma}\sqrt{\frac{1}{n} + \frac{(x_0 - \bar{X})^2}{\sum_{i=1}^n(X_i-\bar{X})^2}}$

Prediction interval SE at $x_0$, $\hat{\sigma}\sqrt{1 + \frac{1}{n} + \frac{(x_0 - \bar{X})^2}{\sum_{i=1}^n(X_i-\bar{X})^2}}$

### Example
```{r}
library(ggplot2);
newx <- data.frame(x = seq(min(x), max(x), length = 100));
p1 <- data.frame(predict(fit, newdata = newx, interval = ("confidence")));
p2 <- data.frame(predict(fit, newdata = newx, interval = ("prediction")));
p1$interval <- "confidence";
p2$interval <- "prediction";
p1$x <- newx$x;
p2$x <- newx$x;
dat <- rbind(p1, p2);
names(dat)[1] <- "y";

g <- ggplot(dat, aes(x = x, y = y));
g <- g + geom_line();
g <- g + geom_point(data = data.frame(x = x, y = y), aes(x =x, y = y), size = 4);
g <- g + geom_ribbon(aes(ymin = lwr, ymax = upr, fill = interval), alpha = 0.2);
g;
```